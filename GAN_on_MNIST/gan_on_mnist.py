#!/use/bin/python
# -*- coding: utf-8 -*-
"""GAN_on_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vWzIMN9j27UUxIx4xF_mTkxeyV2j6mrm

# Preperation
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs
import time
import pickle as pkl
from DatasetWrapper import DataSet, MNISTDataSet
import tensorflow as tf
from tensorflow.python.client import device_lib
import matplotlib.pyplot as plt

# Hyperparameter
from PictureRecorder import PictureRecorder

print(device_lib.list_local_devices())
BATCH_SIZE = 1024
PIC_SIZE = 32
COLORS = 3
EPOCHS = 500

NUM_CLASSES = 10

LATENT_SPACE = 2

recorder = PictureRecorder()

"""# Load Data"""
dataset: DataSet = MNISTDataSet()
x_data, y_data = dataset.get_data(amount=10000)

# onl one class
# class_name = 5
# x_train = x_train[y_train==class_name]
# y_train = y_train[y_train==class_name]

# Batches
train_dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)

dataset.show_example()
""" Create networks"""


class GANDiscriminator:

    def __init__(self, input_dim=(28, 28, 1), hidden_units=100):
        input_layer = tf.keras.Input(shape=input_dim)
        x = tf.keras.layers.Conv2D(64, (5, 5),
                                   strides=(2, 2),
                                   padding='same',
                                   input_shape=input_dim)(input_layer)

        x = tf.keras.layers.LeakyReLU()(x)

        x = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2))(x)
        x = tf.keras.layers.Flatten()(x)
        x = tf.keras.layers.LeakyReLU()(x)

        x_out = tf.keras.layers.Dense(1, activation="softmax")(x)

        self.model = tf.keras.Model(inputs=input_layer, outputs=x_out)

        self.opt = tf.optimizers.Adam(learning_rate=0.0001)
        self.loss_fn = tf.losses.BinaryCrossentropy(from_logits=True)

        # Logging
        self.train_fake_acc_metric = tf.keras.metrics.CategoricalAccuracy()
        self.train_real_acc_metric = tf.keras.metrics.CategoricalAccuracy()

        self.val_acc_metric = tf.keras.metrics.CategoricalAccuracy()

    def __call__(self, x_in, training=True):
        return self.model(x_in, training)

    @tf.function
    def train_step(self, x, y):
        with tf.GradientTape() as grad_tape:
            logits = self(x, training=True)
            loss = self.loss_fn(y, logits)
        grads = grad_tape.gradient(loss, self.model.trainable_weights)
        self.opt.apply_gradients(zip(grads, self.model.trainable_weights))

        self.val_acc_metric.update_state(y, logits)
        return loss

    def reset_metrics(self):
        self.train_fake_acc_metric.reset_metrics()
        self.train_real_acc_metric.reset_metrics()
        self.val_acc_metric.reset_metrics()


class GANGenerator:
    def __init__(self, input_dim=(None, 100)):
        input_layer = tf.keras.Input(shape=input_dim)

        x = tf.keras.layers.Dense(7 * 7 * 256, use_bias=False)(input_layer)
        x = tf.keras.layers.LeakyReLU()(x)

        x = tf.keras.layers.Reshape((7, 7, 256))(x)

        x = tf.keras.layers.Conv2DTranspose(128, (5, 5),
                                            padding="same",
                                            use_bias=False)(x)
        x = tf.keras.layers.LeakyReLU()(x)
        # (7,7,128)

        x = tf.keras.layers.Conv2DTranspose(64, (5, 5),
                                            strides=(2, 2),
                                            padding="same",
                                            use_bias=False)(x)
        x = tf.keras.layers.LeakyReLU()(x)

        # (14,14,128)
        x_out = tf.keras.layers.Conv2DTranspose(1, (5, 5),
                                                strides=(2, 2),
                                                padding="same",
                                                activation='tanh',
                                                use_bias=False)(x)

        self.model = tf.keras.Model(inputs=input_layer, outputs=x_out)

        self.opt = tf.optimizers.Adam(learning_rate=0.0001)
        self.loss_fn = tf.losses.BinaryCrossentropy(from_logits=True)

        # Logging
        self.train_acc_metric = tf.keras.metrics.CategoricalAccuracy()
        self.val_acc_metric = tf.keras.metrics.CategoricalAccuracy()

    def __call__(self, x_in, training=True):
        return self.model(x_in, training)


def get_random_seed(shape=(BATCH_SIZE, 100)):
    """Method to create a random seed"""
    return tf.random.normal(shape=shape)


validation_seed = get_random_seed((BATCH_SIZE, LATENT_SPACE))

gen = GANGenerator(input_dim=(None, LATENT_SPACE))
disc = GANDiscriminator(input_dim=(dataset.img_x_size,
                                   dataset.img_y_size, 1))

for epoch in range(EPOCHS):
    real_data, generated_data = (None, None)
    start_time = time.time()
    for step, (_real_batch, _) in enumerate(train_dataset.shuffle(1024)):
        # # DISCRIMINATOR STEP
        # Get one batch
        length = _real_batch.shape[0]
        # _real_batch = list(train_dataset.shuffle(1024).take(1).as_numpy_iterator())[0][0]
        _real_loss = disc.train_step(_real_batch, tf.ones(shape=(length, 1)))

        # GENERATOR STEP
        with tf.GradientTape() as tape:
            _fake_batch = gen(get_random_seed(shape=(BATCH_SIZE, LATENT_SPACE)))
            y_hat_fake = disc(_fake_batch)
            _gen_loss = gen.loss_fn(tf.ones(shape=(BATCH_SIZE, 1)), y_hat_fake)
        _gen_grad = tape.gradient(_gen_loss, gen.model.trainable_weights)
        gen.opt.apply_gradients(zip(_gen_grad, gen.model.trainable_weights))

        # Learn Fake Disc
        _fake_loss = disc.train_step(_fake_batch, tf.zeros(shape=(BATCH_SIZE, 1)))

    end_time = time.time()

    tf.print(f"EPOCH {epoch} || D_LOSS_REAL: {_real_loss:.8f} "
             f"| D_LOSS_FAKE: {_fake_loss:.8f} |"
             f" G_LOSS: {_gen_loss:.8f} | {end_time - start_time:.2f} s")

    _validation_batch = gen(validation_seed)
    recorder.add(_validation_batch[0].numpy().reshape((28, 28)) * 127.5 + 127.5)

recorder.save_movie("test_latent_2.mp4")
oSaver = tf.saved_model.save(gen.model, "generator_model_latent_10")


if __name__ == '__main__':
    pass
